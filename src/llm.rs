/// Adapted from openai-api, which I'm not using because it's not async and out of date.
use std::collections::HashMap;
use reqwest::{Client, header::AUTHORIZATION, Error};
use serde::{Serialize, Deserialize};
use serde_json::Value;

pub const GPT3_MODEL: &str = "gpt-3.5-turbo-1106";
const COMPLETION_API: &str = "https://api.openai.com/v1/chat/completions";


#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct Usage {
	pub completion_tokens: u32,
	pub prompt_tokens: u32,
	pub total_tokens: u32,
}

#[derive(Clone, Copy, Debug, Serialize, Deserialize)]
#[serde(rename_all = "lowercase")]
pub enum Role {
	System,
	Assistant,
	User,
}

#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct Function {
    pub name: String,
    pub arguments: String,
}

#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct Tool {
    pub id: String,
    pub r#type: String,
    pub function: Function
}

#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct Message {
	pub role: Role,
	pub content: String,
	#[serde(skip_serializing_if = "Option::is_none")]
    pub tool_calls: Option<Vec<Tool>>
}

#[derive(Debug, Serialize, Deserialize)]
pub struct Choice {
    pub finish_reason: String,
	pub index: u32,
	pub logprobs: Option<Value>,
	pub message: Message,
}

/// Given a prompt, the model will return one or more predicted completions,
/// and can also return the probabilities of alternative tokens at each position.
#[derive(Debug, Serialize, Deserialize)]
pub struct Completion {
	pub id: Option<String>,
	pub choices: Vec<Choice>,
	pub created: u64,
	pub model: String,
    pub system_fingerprint: String,
	pub object: String,
	pub usage: Usage,
}

/// Request body for `Create completion` API
#[derive(Debug, Serialize, Deserialize, Default)]
pub struct CompletionsRequest {
	/// ID of the model to use
    /// Required
	pub model: String,

	/// The messages to generate completions for.
    /// Required
	pub messages: Vec<Message>,

	/// Number between -2.0 and 2.0.
	/// Positive values penalize new tokens based on their existing frequency in the text so far,
	/// decreasing the model's likelihood to repeat the same line verbatim.
	/// Defaults to 0
	#[serde(skip_serializing_if = "Option::is_none")]
	pub frequency_penalty: Option<f32>,

    /// Modify the likelihood of specified tokens appearing in the completion.
    /// Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an
    /// associated bias value from -100 to 100. Mathematically, the bias is added to the logits
    /// generated by the model prior to sampling. The exact effect will vary per model, but values
    /// between -1 and 1 should decrease or increase likelihood of selection; values like -100 or
    /// 100 should result in a ban or exclusive selection of the relevant token.
    /// Defaults to null.
	#[serde(skip_serializing_if = "Option::is_none")]
	pub logit_bias: Option<HashMap<String, String>>,

    /// Whether to return log probabilities of the output tokens or not. If true, returns the log
    /// probabilities of each output token returned in the content of message. This option is
    /// currently not available on the gpt-4-vision-preview model.
    /// Defaults to false
	#[serde(skip_serializing_if = "Option::is_none")]
	pub logprobs: Option<bool>,

    /// An integer between 0 and 5 specifying the number of most likely tokens to return at each
    /// token position, each with an associated log probability. logprobs must be set to true if
    /// this parameter is used.
    /// Optional
	#[serde(skip_serializing_if = "Option::is_none")]
    pub top_logprobs: Option<u8>,

    /// The maximum number of tokens that can be generated in the chat completion.
    /// The total length of input tokens and generated tokens is limited by the model's context
    /// length. Example Python code for counting tokens.
    /// Optional
	#[serde(skip_serializing_if = "Option::is_none")]
	pub max_tokens: Option<u32>,

    /// How many chat completion choices to generate for each input message. Note that you will be
    /// charged based on the number of generated tokens across all of the choices. Keep n as 1 to
    /// minimize costs. Defaults to 1
	#[serde(skip_serializing_if = "Option::is_none")]
	pub n: Option<u32>,

    /// Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they
    /// appear in the text so far, increasing the model's likelihood to talk about new topics.
    /// Defaults to 0
	#[serde(skip_serializing_if = "Option::is_none")]
	pub presence_penalty: Option<f32>,

    /// An object specifying the format that the model must output. Compatible with
    /// gpt-4-1106-preview and gpt-3.5-turbo-1106. Setting to { "type": "json_object" } enables
    /// JSON mode, which guarantees the message the model generates is valid JSON. Important: when
    /// using JSON mode, you must also instruct the model to produce JSON yourself via a system or
    /// user message. Without this, the model may generate an unending stream of whitespace until
    /// the generation reaches the token limit, resulting in a long-running and seemingly "stuck"
    /// request. Also note that the message content may be partially cut off if
    /// finish_reason="length", which indicates the generation exceeded max_tokens or the
    /// conversation exceeded the max context length.
	#[serde(skip_serializing_if = "Option::is_none")]
    pub response_format: Option<Value>,

    /// This feature is in Beta. If specified, our system will make a best effort to sample
    /// deterministically, such that repeated requests with the same seed and parameters should
    /// return the same result. Determinism is not guaranteed, and you should refer to the
    /// system_fingerprint response parameter to monitor changes in the backend.
    /// Optional. If specified, the system will attempt to provide deterministic results
    /// using this seed. Note: Determinism is not guaranteed.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub seed: Option<i32>,

    /// Up to 4 sequences where the API will stop generating further tokens.
    /// Optional. Defaults to null.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub stop: Option<Vec<String>>,

    /// Boolean indicating whether to stream partial message deltas.
    /// Optional. Defaults to false.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub stream: Option<bool>,

    /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the
    /// output more random, while lower values like 0.2 will make it more focused and
    /// deterministic. We generally recommend altering this or top_p but not both.
    /// Defaults to 1.0
    #[serde(skip_serializing_if = "Option::is_none")]
    pub temperature: Option<f32>,

    /// An alternative to sampling with temperature, called nucleus sampling, where the model
    /// considers the results of the tokens with top_p probability mass. So 0.1 means only the
    /// tokens comprising the top 10% probability mass are considered. We generally recommend
    /// altering this or temperature but not both.
    /// Defaults to 1.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub top_p: Option<f32>,

    /// A list of tools the model may call. Currently, only functions are supported as a tool. Use
    /// this to provide a list of functions the model may generate JSON inputs for.
    /// Optional.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tools: Option<Vec<Value>>,

    /// Controls which (if any) function is called by the model. none means the model will not call
    /// a function and instead generates a message. auto means the model can pick between
    /// generating a message or calling a function. Specifying a particular function via {"type":
    /// "function", "function": {"name": "my_function"}} forces the model to call that function.
    /// Defaults to 'none' if no functions are present, 'auto' if functions are present.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tool_choice: Option<Value>,

    /// A unique identifier representing your end-user, which can help OpenAI to monitor and detect
    /// abuse.
    /// Optional
    #[serde(skip_serializing_if = "Option::is_none")]
    pub user: Option<String>
}

pub async fn completion(client: &Client,
                        auth_token: &str,
                        completion: CompletionsRequest)
    -> Result<Completion, Error>
{
    tracing::warn!("compl: {}", serde_json::to_string(&completion).unwrap());
    let response = client.post(COMPLETION_API)
        .json(&completion)
        .header(AUTHORIZATION, &format!("Bearer {}", auth_token))
        .send()
        .await?;
    let response = match response.error_for_status() {
        Ok(response) => response,
        Err(err) => {
            tracing::error!("API error: {}", err);
            return Err(err);
        }
    };
    match response.json().await {
        Ok(response) => Ok(response),
        Err(err) => {
            tracing::error!("Deserialization error: {}", err);
            Err(err)
        }
    }
}
